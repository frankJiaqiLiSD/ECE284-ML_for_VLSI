{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "radical-fifty",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "radical-fifty",
        "outputId": "b28d05c7-0879-4d31-b1fd-0aa434dcbcc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Building model...\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Importign VggNET model. Only importing one mode since using Colab.\n",
        "from vgg import *\n",
        "\n",
        "global best_prec\n",
        "use_gpu = torch.cuda.is_available()\n",
        "print('=> Building model...')\n",
        "\n",
        "# vgg normally runs slower than ResNET, so I chose to inscrease batch size to make it use less computational resource.\n",
        "batch_size = 256\n",
        "\n",
        "model_name = \"cifar10\"\n",
        "#model = cifar10()\n",
        "model = VGG16()\n",
        "\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "print_freq = 40 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
        "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
        "\n",
        "def train(trainloader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter()   ## at the begining of each epoch, this should be reset\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()  # measure current time\n",
        "\n",
        "    for i, (input, target) in enumerate(trainloader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)  # data loading time\n",
        "\n",
        "        input, target = input.cuda(), target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec = accuracy(output, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec.item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end) # time spent to process one batch\n",
        "        end = time.time()\n",
        "\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
        "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion ):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "\n",
        "            input, target = input.cuda(), target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec = accuracy(output, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
        "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                   top1=top1))\n",
        "\n",
        "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
        "    return top1.avg\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,5)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk) # 5\n",
        "    batch_size = target.size(0) # 128\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True) # topk(k, dim=None, largest=True, sorted=True)\n",
        "                                    # will output (max value, its index)\n",
        "    pred = pred.t()               # transpose\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))   # \"-1\": calculate automatically\n",
        "\n",
        "    res = []\n",
        "    for k in topk: # 1, 5\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)  # view(-1): make a flattened 1D tensor\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))   # correct: size of [maxk, batch_size]\n",
        "    return res\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n    ## n is impact factor\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, fdir):\n",
        "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
        "    torch.save(state, filepath)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
        "    adjust_list = [20, 25, 27]\n",
        "    if epoch in adjust_list:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * 0.1\n",
        "\n",
        "#model = nn.DataParallel(model).cuda()\n",
        "#all_params = checkpoint['state_dict']\n",
        "#model.load_state_dict(all_params, strict=False)\n",
        "#criterion = nn.CrossEntropyLoss().cuda()\n",
        "#validate(testloader, model, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "welsh-sample",
      "metadata": {
        "id": "welsh-sample"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter) ## If you run this line, the next data batch is called subsequently.\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "small-favorite",
      "metadata": {
        "id": "small-favorite"
      },
      "outputs": [],
      "source": [
        "# This cell is from the website\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "\n",
        "weight_decay = 1e-3\n",
        "epochs = 30\n",
        "best_prec = 0\n",
        "\n",
        "model = model.cuda()\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "# weight decay: for regularization to prevent overfitting\n",
        "\n",
        "\n",
        "if not os.path.exists('result'):\n",
        "    os.makedirs('result')\n",
        "\n",
        "fdir = 'result/'+str(model_name)\n",
        "\n",
        "if not os.path.exists(fdir):\n",
        "    os.makedirs(fdir)\n",
        "\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "    train(trainloader, model, criterion, optimizer, epoch)\n",
        "\n",
        "    # evaluate on test set\n",
        "    print(\"Validation starts\")\n",
        "    prec = validate(testloader, model, criterion)\n",
        "\n",
        "    # remember best precision and save checkpoint\n",
        "    is_best = prec > best_prec\n",
        "    best_prec = max(prec,best_prec)\n",
        "    print('best acc: {:1f}'.format(best_prec))\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_prec': best_prec,\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }, is_best, fdir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "hydraulic-passport",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hydraulic-passport",
        "outputId": "17172935-d1e4-42dc-9d71-ccb64d6ab529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: [0/40]\tTime 0.554 (0.554)\tLoss 0.2821 (0.2821)\tPrec 92.578% (92.578%)\n",
            " * Prec 91.540% \n"
          ]
        }
      ],
      "source": [
        "\n",
        "fdir = 'result/'+str(model_name)+'/model_best.pth.tar'\n",
        "\n",
        "checkpoint = torch.load(fdir)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "model.eval()\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "prec = validate(testloader, model, criterion)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}